{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing os and load_dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dotenv and getting the openai api key from the .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PyPDFLoader from langchain.document_loaders\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\genai_project\\\\Interview-Questions-Generator\\\\research'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the present working directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\genai_project\\Interview-Questions-Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\genai_project\\Interview-Questions-Generator\\inter\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\genai_project\\\\Interview-Questions-Generator'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Ingesting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the file daved into a variable\n",
    "# calling the PyPDFLoader() class and creating an object\n",
    "# and through the object call a function call functioned as load()\n",
    "file_path = 'data/stats.pdf'\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance\n",
      "and\n",
      "the\n",
      "use\n",
      "of\n",
      "correlation\n",
      "in\n",
      "Statistics\n",
      "Introduction\n",
      "Correlation\n",
      "is\n",
      "a\n",
      "statistical\n",
      "measure\n",
      "that\n",
      "expresses\n",
      "the\n",
      "extent\n",
      "to\n",
      "which\n",
      "two\n",
      "variables\n",
      "are\n",
      "linearly\n",
      "related.\n",
      "It\n",
      "is\n",
      "a\n",
      "common\n",
      "tool\n",
      "for\n",
      "describing\n",
      "simple\n",
      "relationships\n",
      "without\n",
      "making\n",
      "a\n",
      "statement\n",
      "about\n",
      "cause\n",
      "and\n",
      "effect.\n",
      "Correlation\n",
      "coefficients\n",
      "range\n",
      "from\n",
      "-1\n",
      "to\n",
      "1,\n",
      "with\n",
      "a\n",
      "value\n",
      "of\n",
      "0\n",
      "indicating\n",
      "no\n",
      "linear\n",
      "relationship\n",
      "between\n",
      "the\n",
      "two\n",
      "variables,\n",
      "a\n",
      "value\n",
      "of\n",
      "1\n",
      "indicating\n",
      "a\n",
      "perfect\n",
      "positive\n",
      "linear\n",
      "relationship,\n",
      "and\n",
      "a\n",
      "value\n",
      "of\n",
      "-1\n",
      "indicating\n",
      "a\n",
      "perfect\n",
      "negative\n",
      "linear\n",
      "relationship.\n",
      "Correlation\n",
      "is\n",
      "important\n",
      "in\n",
      "statistics\n",
      "because\n",
      "it\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "1.\n",
      "Identify\n",
      "relationships\n",
      "between\n",
      "variables:\n",
      "Correlation\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "identify\n",
      "whether\n",
      "there\n",
      "is\n",
      "a\n",
      "relationship\n",
      "between\n",
      "two\n",
      "variables,\n",
      "and\n",
      "if\n",
      "so,\n",
      "whether\n",
      "the\n",
      "relationship\n",
      "is\n",
      "positive\n",
      "or\n",
      "negative.\n",
      "This\n",
      "information\n",
      "can\n",
      "be\n",
      "useful\n",
      "for\n",
      "understanding\n",
      "the\n",
      "relationships\n",
      "between\n",
      "different\n",
      "factors\n",
      "in\n",
      "a\n",
      "complex\n",
      "system.\n",
      "2.\n",
      "Make\n",
      "predictions:\n",
      "If\n",
      "there\n",
      "is\n",
      "a\n",
      "strong\n",
      "correlation\n",
      "between\n",
      "two\n",
      "variables,\n",
      "it\n",
      "is\n",
      "possible\n",
      "to\n",
      "use\n",
      "the\n",
      "value\n",
      "of\n",
      "one\n",
      "variable\n",
      "to\n",
      "predict\n",
      "the\n",
      "value\n",
      "of\n",
      "the\n",
      "other\n",
      "variable.\n",
      "This\n",
      "can\n",
      "be\n",
      "useful\n",
      "for\n",
      "making\n",
      "predictions\n",
      "in\n",
      "a\n",
      "variety\n",
      "of\n",
      "fields,\n",
      "such\n",
      "as\n",
      "business,\n",
      "finance,\n",
      "and\n",
      "medicine.\n",
      "3.\n",
      "Develop\n",
      "causal\n",
      "models:\n",
      "Correlation\n",
      "can\n",
      "be\n",
      "used\n",
      "as\n",
      "a\n",
      "starting\n",
      "point\n",
      "for\n",
      "developing\n",
      "causal\n",
      "models,\n",
      "which\n",
      "are\n",
      "models\n",
      "that\n",
      "describe\n",
      "how\n",
      "changes\n",
      "in\n",
      "one\n",
      "variable\n",
      "cause\n",
      "changes\n",
      "in\n",
      "other\n",
      "variables.\n",
      "Causal\n",
      "models\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "make\n",
      "more\n",
      "accurate\n",
      "predictions\n",
      "and\n",
      "to\n",
      "develop\n",
      "interventions\n",
      "to\n",
      "change\n",
      "the\n",
      "values\n",
      "of\n",
      "specific\n",
      "variables.\n",
      "Correlation\n",
      "is\n",
      "used\n",
      "in\n",
      "a\n",
      "wide\n",
      "variety\n",
      "of\n",
      "fields\n",
      "including\n",
      "1.\n",
      "Business:\n",
      "Correlation\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "identify\n",
      "relationships\n",
      "between\n",
      "different\n",
      "business\n",
      "variables,\n",
      "such\n",
      "as\n",
      "sales,\n",
      "advertising\n",
      "spending,\n",
      "and\n",
      "customer\n",
      "satisfaction.\n",
      "This\n",
      "information\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "make\n",
      "better\n",
      "business\n",
      "decisions,\n",
      "such\n",
      "as\n",
      "how\n",
      "to\n",
      "allocate\n",
      "marketing\n",
      "resources.\n",
      "2.\n",
      "Finance:\n",
      "Correlation\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "identify\n",
      "relationships\n",
      "between\n",
      "different\n",
      "financial\n",
      "assets,\n",
      "such\n",
      "as\n",
      "stocks,\n",
      "bonds,\n",
      "and\n",
      "commodities.\n",
      "This\n",
      "information\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "build\n",
      "diversified\n",
      "portfolios\n",
      "that\n",
      "reduce\n",
      "risk.\n",
      "3.\n",
      "Medicine:\n",
      "Correlation\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "identify\n",
      "relationships\n",
      "between\n",
      "different\n",
      "medical\n",
      "variables,\n",
      "such\n",
      "as\n",
      "risk\n",
      "factors\n",
      "for\n",
      "diseases\n",
      "and\n",
      "the\n",
      "effectiveness\n",
      "of\n",
      "treatments.\n",
      "This\n",
      "information\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "improve\n",
      "the\n",
      "prevention,\n",
      "diagnosis,\n",
      "and\n",
      "treatment\n",
      "of\n",
      "diseases.\n",
      "4.\n",
      "Psychology:\n",
      "Correlation\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "identify\n",
      "relationships\n",
      "between\n",
      "different\n",
      "psychological\n",
      "variables,\n",
      "such\n",
      "as\n",
      "personality\n",
      "traits,\n",
      "cognitive\n",
      "abilities,\n",
      "and\n",
      "mental\n",
      "disorders.\n",
      "This\n",
      "information\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "develop\n",
      "better\n",
      "psychological\n",
      "assessments\n",
      "and\n",
      "treatments.\n",
      "How\n",
      "correlation\n",
      "is\n",
      "used\n",
      "in\n",
      "the\n",
      "real\n",
      "world\n",
      "1.\n",
      "A\n",
      "marketing\n",
      "manager\n",
      "might\n",
      "use\n",
      "correlation\n",
      "to\n",
      "identify\n",
      "the\n",
      "relationship\n",
      "between\n",
      "advertising\n",
      "spending\n",
      "and\n",
      "sales.\n",
      "This\n",
      "information\n",
      "could\n",
      "be\n",
      "used\n",
      "to\n",
      "decide\n",
      "how\n",
      "much\n",
      "money\n",
      "to\n",
      "allocate\n",
      "to\n",
      "advertising.\n",
      "2.\n",
      "A\n",
      "financial\n",
      "analyst\n",
      "might\n",
      "use\n",
      "correlation\n",
      "to\n",
      "identify\n",
      "the\n",
      "relationship\n",
      "between\n",
      "the\n",
      "returns\n",
      "of\n",
      "different\n",
      "stocks.\n",
      "This\n",
      "information\n",
      "could\n",
      "be\n",
      "used\n",
      "to\n",
      "build\n",
      "a\n",
      "portfolio\n",
      "of\n",
      "stocks\n",
      "that\n",
      "is\n",
      "diversified\n",
      "and\n",
      "has\n",
      "a\n",
      "lower\n",
      "overall\n",
      "risk.\n",
      "3.\n",
      "A\n",
      "medical\n",
      "researcher\n",
      "might\n",
      "use\n",
      "correlation\n",
      "to\n",
      "identify\n",
      "the\n",
      "relationship\n",
      "between\n",
      "smoking\n",
      "and\n",
      "lung\n",
      "cancer.\n",
      "This\n",
      "information\n",
      "could\n",
      "be\n",
      "used\n",
      "to\n",
      "develop\n",
      "public\n",
      "health\n",
      "campaigns\n",
      "to\n",
      "discourage\n",
      "smoking.\n",
      "4.\n",
      "A\n",
      "psychologist\n",
      "might\n",
      "use\n",
      "correlation\n",
      "to\n",
      "identify\n",
      "the\n",
      "relationship\n",
      "between\n",
      "anxiety\n",
      "and\n",
      "depression.\n",
      "This\n",
      "information\n",
      "could\n",
      "be\n",
      "used\n",
      "to\n",
      "develop\n",
      "more\n",
      "effective\n",
      "treatments\n",
      "for\n",
      "anxiety\n",
      "and\n",
      "depression.\n",
      "Conclusion\n",
      "It\n",
      "is\n",
      "important\n",
      "to\n",
      "note\n",
      "that\n",
      "correlation\n",
      "does\n",
      "not\n",
      "equal\n",
      "causation.\n",
      "Just\n",
      "because\n",
      "two\n",
      "variables\n",
      "are\n",
      "correlated\n",
      "does\n",
      "not\n",
      "mean\n",
      "that\n",
      "one\n",
      "variable\n",
      "causes\n",
      "the\n",
      "other .\n",
      "For\n",
      "example,\n",
      "there\n",
      "is\n",
      "a\n",
      "strong\n",
      "correlation\n",
      "between\n",
      "ice\n",
      "cream\n",
      "sales\n",
      "and\n",
      "shark\n",
      "attacks.\n",
      "However ,\n",
      "this\n",
      "does\n",
      "not\n",
      "mean\n",
      "that\n",
      "eating\n",
      "ice\n",
      "cream\n",
      "causes\n",
      "shark\n",
      "attacks.\n",
      "Instead,\n",
      "there\n",
      "is\n",
      "likely\n",
      "a\n",
      "third\n",
      "variable,\n",
      "such\n",
      "as\n",
      "hot\n",
      "weather ,\n",
      "that\n",
      "causes\n",
      "both\n",
      "ice\n",
      "cream\n",
      "sales\n",
      "and\n",
      "shark\n",
      "attacks\n",
      "to\n",
      "increase.\n",
      "Overall,\n",
      "correlation\n",
      "is\n",
      "a\n",
      "powerful\n",
      "statistical\n",
      "tool\n",
      "that\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "identify\n",
      "relationships\n",
      "between\n",
      "variables,\n",
      "make\n",
      "predictions,\n",
      "and\n",
      "develop\n",
      "causal\n",
      "models.\n",
      "It\n",
      "is\n",
      "used\n",
      "in\n",
      "a\n",
      "wide\n",
      "variety\n",
      "of\n",
      "fields\n",
      "to\n",
      "make\n",
      "better\n",
      "decisions\n",
      "and\n",
      "improve\n",
      "outcomes.\n"
     ]
    }
   ],
   "source": [
    "# showing the whole dataset\n",
    "data\n",
    "# showing the data through indexing\n",
    "data[0]\n",
    "# length of the dataset\n",
    "len(data)\n",
    "# printint the dataset through length wise\n",
    "for i in data:\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the all page content of the whole dataset into a string varible\n",
    "question_gen = ''\n",
    "\n",
    "for page in data:\n",
    "    question_gen += page.page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-2 : Chunking \n",
    "#### means partioning text into chunking giving chunk_size and chunk_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing TokenTextSplitter from langchain.text_splitter. we need it for\n",
    "# splitting the whole documents because every model has specific token limitations\n",
    "# or we can say chunk_size\n",
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will call the TokenTextSplitter() class and making an object of it\n",
    "splitter_ques_gen = TokenTextSplitter(\n",
    "    model_name= \"gpt-3.5-turbo\",\n",
    "    chunk_size= 1000,\n",
    "    chunk_overlap = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function split_text() through the object we just created\n",
    "# and passing the question_gen where all data are saved into single string varible.\n",
    "chunk_ques_gen = splitter_ques_gen.split_text(question_gen)\n",
    "# now our whole dataset saved into a list inside the string type data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Document from langchain.docstore.document\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to convert the data into documets type\n",
    "# and this is the procedure to convert the data into a list\n",
    "document_ques_gen = [Document(page_content = t) for t in chunk_ques_gen]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an obejct of TokenTextSplitter() class\n",
    "# why we are doing the same thing because if we check the previous splitter,\n",
    "# the length of the list is 1\n",
    "# and we use 10000 for chunk size.\n",
    "# now we will take smaller chunks size and splitting the text\n",
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name = 'gpt-3.5-turbo',\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling split_documents() function through the object splitter_ans_gen\n",
    "document_answer_gen = splitter_ans_gen.split_documents(\n",
    "    document_ques_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Importance\\nand\\nthe\\nuse\\nof\\ncorrelation\\nin\\nStatistics\\nIntroduction\\nCorrelation\\nis\\na\\nstatistical\\nmeasure\\nthat\\nexpresses\\nthe\\nextent\\nto\\nwhich\\ntwo\\nvariables\\nare\\nlinearly\\nrelated.\\nIt\\nis\\na\\ncommon\\ntool\\nfor\\ndescribing\\nsimple\\nrelationships\\nwithout\\nmaking\\na\\nstatement\\nabout\\ncause\\nand\\neffect.\\nCorrelation\\ncoefficients\\nrange\\nfrom'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting first index of the documents through the page_content attribute\n",
    "document_answer_gen[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the document\n",
    "len(document_answer_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing ChatOpenAI from from langchain.chat_models\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warnings while running the cell\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object of ChatOpenAI() class\n",
    "llm_ques_gen_pipeline = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template designing\n",
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PromptTemplate from langchain.prompts\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function PromptTemplate() and passing few parameters \n",
    "# and saved into a varible called as PROMPT_QUESTIONS\n",
    "PROMPT_QUESTIONS = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=['text']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prompt template is designed for another concept of refinig or making meaningful answers\n",
    "refine_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function PromptTemplate() and passing few parameters \n",
    "# and saved into a varible called as REFINE_PROMPT_QUESTIONS\n",
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing load_summarize_chain from langchain.chains.summarize\n",
    "# here load_summarize_chain would be either class or function\n",
    "# and langchain.chains.summarize is coming from the path langchain/chains/summarize\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling load_summarize_chain() function and passing few parameters\n",
    "ques_gen_chain = load_summarize_chain(\n",
    "    llm = llm_ques_gen_pipeline, \n",
    "    chain_type = \"refine\", \n",
    "    verbose = True, \n",
    "    question_prompt=PROMPT_QUESTIONS, \n",
    "    refine_prompt=REFINE_PROMPT_QUESTIONS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these cell will not work because of openai 429 quota issues.\n",
    "# if you want to access this cell, u need to have paid openai account.\n",
    "ques = ques_gen_chain.run(document_ques_gen)\n",
    "print(ques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3 : Embeddings\n",
    "#### we used here OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing OpenAIEmbeddings from langchain.embeddings.openai\n",
    "# the path is langchain/embeddings/openai and \n",
    "# OpenAIEmbeddings is either class or function\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object of OpenAIEmbeddings() class\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4 : Data stored into Vector Daabase\n",
    "#### texts are converted into numbers and these numbers are called vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the vectors, we need to have a vector database and here we\n",
    "# used FAISS vector database system and this is offline database. \n",
    "# it used for efficient similarity search and clustering of large datasets of vectors.\n",
    "# importing FAISS and this is a class\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating object as vector_store of  FAISS class \n",
    "# and calling a function through of it.\n",
    "# passing params as document_answer_gen and embeddings model\n",
    "vector_store = FAISS.from_documents(document_answer_gen, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we need to pass the answers to the ChatOpenAI module.\n",
    "# so we call the ChatOpenAI() module\n",
    "llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my question\n",
    "ques\n",
    "\n",
    "# we need to see the question as list\n",
    "# we split the question through split() function and we splitted by \\n\n",
    "ques_list = ques.split(\"\\n\")\n",
    "\n",
    "# here are the questions and sshowing inside a list\n",
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need answers for those question that just created. For this\n",
    "# reason, we need ti import RetrievalQA module from the path langchain.chains\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating object of RetrievalQA() class and \n",
    "# calling function from_chain_type() through the object and \n",
    "# passing few parameters.\n",
    "answer_generation_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_answer_gen, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vector_store.as_retriever()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each question and save to a file\n",
    "for question in ques_list:\n",
    "    print(\"Question: \", question)\n",
    "    answer = answer_generation_chain.run(question)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"--------------------------------------------------\\\\n\\\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\\\n\")\n",
    "        f.write(\"--------------------------------------------------\\\\n\\\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
