{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is mehedi\n"
     ]
    }
   ],
   "source": [
    "print('this is mehedi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing os and load_dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dotenv and getting the openai api key from the .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PyPDFLoader from langchain.document_loaders\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\genai_project\\\\Interview-Questions-Generator\\\\research'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the present working directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gettingone step back from the present working directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\genai_project\\\\Interview-Questions-Generator'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the present working directory and we are one step back\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the file daved into a variable\n",
    "# calling the PyPDFLoader() class and creating an object\n",
    "# and through the object call a function call functioned as load()\n",
    "file_path = 'data/SDG.pdf'\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the whole dataset\n",
    "data\n",
    "# showing the data through indexing\n",
    "data[0]\n",
    "# length of the dataset\n",
    "len(data)\n",
    "# printint the dataset through length wise\n",
    "for i in data:\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the all page content of the whole dataset into a varible\n",
    "final_data=''\n",
    "for i in data:\n",
    "    final_data += i.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing TokenTextSplitter from langchain.text_splitter. we need it for\n",
    "# splitting the whole documents because every model has specific token limitations\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# now we will call the TokenTextSplitter() class and making an object of it\n",
    "splitted_docs = TokenTextSplitter(\n",
    "    model_name= \"gpt-3.5-turbo\",\n",
    "    chunk_size= 10000,\n",
    "    chunk_overlap = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function split_text() through the object we just created\n",
    "# and passing the final_docs\n",
    "chunked_docs = splitted_docs.split_text(final_data)\n",
    "# len(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Document from langchain.docstore.document\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehension\n",
    "documents = [Document(page_content = t) for t in chunked_docs]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the whole dataset\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an obejct of TokenTextSplitter() class\n",
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name = 'gpt-3.5-turbo',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling split_documents() function through the object\n",
    "document_answer_gen = splitter_ans_gen.split_documents(\n",
    "    documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_answer_gen\n",
    "len(document_answer_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing ChatOpenAI from from langchain.chat_models\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object of ChatOpenAI() class\n",
    "llm_ques_gen_pipeline = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt designing\n",
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PromptTemplate from langchain.prompts\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function PromptTemplate() and passing few parameters\n",
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the refinig the template\n",
    "\n",
    "final_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling PromptTemplate() function and saving into a variable\n",
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing load_summarize_chain from langchain.chains.summarize\n",
    "# here load_summarize_chain would be either class or function\n",
    "# and langchain.chains.summarize is coming from the path langchain/chains/summarize\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling load_summarize_chain() function\n",
    "ques_gen_chain = load_summarize_chain(\n",
    "    llm = llm_ques_gen_pipeline, \n",
    "    chain_type = \"refine\", \n",
    "    verbose = True, \n",
    "    question_prompt=PROMPT_QUESTIONS, \n",
    "    refine_prompt=REFINE_PROMPT_QUESTIONS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline\n",
    "# ques = ques_gen_chain.run(documents)\n",
    "# print(ques)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
